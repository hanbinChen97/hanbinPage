# Select the LLM provider: "openai", "azure", "gemini", "huggingface"
LLM_PROVIDER="openai"

# OpenAI Configuration
OPENAI_API_KEY="sk-..." # Your OpenAI API Key
OPENAI_MODEL_ID="gpt-3.5-turbo" # Optional: Specify the model ID (defaults to gpt-3.5-turbo)

# Azure OpenAI Configuration (only needed if LLM_PROVIDER="azure")
AZURE_OPENAI_API_KEY="..."
AZURE_OPENAI_ENDPOINT="..." # e.g., https://your-resource-name.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME="..." # The 'Deployment name' of your model in Azure AI Studio

# Google Gemini Configuration (only needed if LLM_PROVIDER="gemini")
GOOGLE_GENERATIVE_AI_API_KEY="..." # Your Google API Key for Gemini
GEMINI_MODEL_ID="gemini-1.5-flash" # Optional: Specify the Gemini model (defaults to gemini-1.5-flash)

# Hugging Face Configuration (only needed if LLM_PROVIDER="huggingface")
# For models compatible with the Vercel AI SDK (e.g., via Inference Endpoints or TGI)
HF_API_KEY="hf_..." # Your Hugging Face API Key (if required by the endpoint)
HF_MODEL_ENDPOINT="https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct" # Example: Llama 3 8B Instruct. Replace with your desired model endpoint.
HF_MODEL_ID="meta-llama/Meta-Llama-3-8B-Instruct" # Optional: Model ID, if needed by the endpoint or for clarity. Can often be derived from endpoint.